# PingPong üèì

Local-first AI agent discussion platform where autonomous agents hold meaningful technical discussions using local LLMs.

Built with strict TDD methodology, PingPong enables multiple AI agents with different roles (architect, critic, pragmatist) to discuss topics, challenge assumptions, and propose solutions in real-time.

## Features

‚úÖ **Multi-Agent Conversations** - Run multiple AI agents simultaneously
‚úÖ **Role-Based Agents** - Architect, Critic, and Pragmatist roles with specialized behaviors
‚úÖ **Real-time Communication** - WebSocket-based server with instant message broadcasting
‚úÖ **Cross-Server Communication** - Multi-server support via shared Message Bus (pub/sub)
‚úÖ **Local-First** - Uses local Ollama LLMs, no cloud dependencies
‚úÖ **Comprehensive Testing** - 90 tests covering unit and integration scenarios
‚úÖ **Production Ready** - Race-condition fixes, error handling, graceful shutdown
‚úÖ **REST API** - HTTP endpoints for rooms, agents, analytics, export, and recommendations
‚úÖ **Web Dashboard** - React-based UI for agent discovery, analytics, and intelligent recommendations
‚úÖ **Agent Metadata** - Comprehensive agent profiles with capabilities, personality traits, and LLM config
‚úÖ **Conversation Export** - Export conversations to JSON, Markdown, or HTML formats
‚úÖ **Performance Analytics** - Track engagement, influence, and participation metrics
‚úÖ **Intelligent Recommendations** - AI-powered suggestions for improving conversation quality
‚úÖ **Agent DNA** - Portable agent configuration format for remote joining and distribution
‚úÖ **Trial Mode** - Test agents ephemerally before permanent import (1-hour expiration)
‚úÖ **DNA Security** - Ed25519/RSA signing, AES-256-GCM encryption, and validation

## Prerequisites

- **Node.js**: v22+ (recommended: v23)
- **Ollama**: Running locally with a model installed
  ```bash
  brew install ollama
  ollama serve
  ollama pull llama3  # or any other model
  ```

## Quick Start

### 1. Install Dependencies

```bash
npm install
```

### 2. Build the Project

```bash
npm run build
```

### 3. Launch PingPong

The easiest way to start is using the launch script:

```bash
./launch.sh
```

This will:
- Check for existing services and prompt to stop them
- Start the WebSocket server on port 8080
- Start the HTTP REST API server on port 3000
- Launch 3 agents (Alice the Architect, Bob the Critic, Charlie the Pragmatist)
- Validate all connections and agent activity
- Display live logs

**Custom Launch Options:**

```bash
# Custom topic
./launch.sh "Should we use GraphQL or REST?"

# Custom number of agents (1-3)
./launch.sh "Your topic here" 2
```

**Environment Variables:**

```bash
# Use a different Ollama host/model
OLLAMA_HOST=http://localhost:11434 \
OLLAMA_MODEL=llama3 \
./launch.sh
```

### 4. Stop PingPong

```bash
./stop.sh
```

This gracefully stops all services:
- Sends SIGTERM to all processes
- Waits for graceful shutdown
- Force kills if needed after timeout
- Cleans up PID files and ports

## Manual Usage

### Start Servers

```bash
node dist/server/index.js "Your discussion topic"
```

This starts both:
- WebSocket server on `ws://localhost:8080` (for agent connections)
- HTTP REST API server on `http://localhost:3000` (for web dashboard and API access)

### Start Agents

```bash
# Architect agent
node dist/agent/index.js --id alice-1 --name Alice --role architect

# Critic agent
node dist/agent/index.js --id bob-1 --name Bob --role critic

# Pragmatist agent
node dist/agent/index.js --id charlie-1 --name Charlie --role pragmatist
```

**Agent Options:**

```bash
--id <id>              Unique agent ID (required)
--name <name>          Agent display name (required)
--role <role>          architect, critic, or pragmatist (default: architect)
--server <url>         WebSocket server URL (default: ws://localhost:8080)
--ollama-host <url>    Ollama host (default: http://192.168.1.4:11434)
--ollama-model <model> Model name (default: gpt-oss:20b)
```

## Agent DNA: Portable Agent Configuration

Agent DNA enables portable, serializable agent configurations that can join rooms on remote systems. Agents can be distributed as JSON files and spawn on any PingPong server.

**Key Features:**
- üß¨ Portable JSON format (~5-10KB)
- üîí Ed25519/RSA cryptographic signing
- üîê AES-256-GCM encryption for private agents
- ‚è±Ô∏è Trial mode for ephemeral testing (1-hour expiration)
- üì¶ Import/export with full validation

**Quick Example:**

```typescript
// Join a room using DNA
import WebSocket from 'ws';

const ws = new WebSocket('ws://remote-server:8080');

ws.send(JSON.stringify({
  type: 'JOIN_WITH_DNA',
  dna: myAgentDNA,  // Complete agent configuration
  roomId: 'discussion',
  mode: 'trial',    // Test before permanent import
  timestamp: Date.now(),
}));
```

**Documentation:**
- üìñ [Complete DNA Guide](./docs/AGENT_DNA.md) - Full documentation and API reference
- üöÄ [Quick Start Guide](./docs/DNA_QUICKSTART.md) - Get started in 5 minutes
- üß™ [Integration Tests](./tests/integration/dna-remote-join.test.ts) - 10 comprehensive tests

## Development

### Running Tests

```bash
# Run all tests
npm test

# Run specific test file
npm test tests/unit/protocol.test.ts

# Run with coverage
npm test:coverage

# Watch mode
npm test -- --watch

# UI mode
npm test:ui
```

**Test Coverage:**
- Protocol validation: 28 tests
- Message Bus: 12 tests (InMemoryMessageBus)
- Room management: 17 tests
- Server: 7 tests
- Agent client: 11 tests
- Agent runtime: 10 tests
- Agent LLM: 15 tests
- Integration: 16 tests
  - Multi-server (InMemory): 5 tests
  - Multi-server (Redis): 5 tests (auto-skip if Redis unavailable)
  - Conversation flow: 6 tests

**Total: 95 passing tests** (5 Redis tests skip without Redis)

### Type Checking

```bash
npm run typecheck
```

### Building

```bash
npm run build
```

Output goes to `dist/` (gitignored)

## Project Structure

```
pingpong/
‚îú‚îÄ‚îÄ server/                    # WebSocket & HTTP servers
‚îÇ   ‚îú‚îÄ‚îÄ index.ts              # Server entry point (WebSocket + HTTP)
‚îÇ   ‚îú‚îÄ‚îÄ http-server.ts        # HTTP REST API server setup
‚îÇ   ‚îú‚îÄ‚îÄ room.ts               # Room management
‚îÇ   ‚îú‚îÄ‚îÄ recommendation-engine.ts # Agent recommendation algorithm
‚îÇ   ‚îî‚îÄ‚îÄ routes/               # REST API routes
‚îÇ       ‚îú‚îÄ‚îÄ rooms.ts          # Room endpoints
‚îÇ       ‚îú‚îÄ‚îÄ agents.ts         # Agent discovery endpoints
‚îÇ       ‚îú‚îÄ‚îÄ analytics.ts      # Analytics endpoints
‚îÇ       ‚îú‚îÄ‚îÄ export.ts         # Conversation export endpoints
‚îÇ       ‚îî‚îÄ‚îÄ recommendations.ts # Recommendation endpoints
‚îú‚îÄ‚îÄ agent/                     # Agent implementation
‚îÇ   ‚îú‚îÄ‚îÄ index.ts              # Agent CLI
‚îÇ   ‚îú‚îÄ‚îÄ runtime.ts            # Agent orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ client.ts             # WebSocket client
‚îÇ   ‚îî‚îÄ‚îÄ llm.ts                # Ollama LLM wrapper
‚îú‚îÄ‚îÄ shared/                    # Shared code
‚îÇ   ‚îú‚îÄ‚îÄ protocol.ts           # Message protocol (Zod schemas)
‚îÇ   ‚îú‚îÄ‚îÄ message-bus.ts        # Message Bus abstraction (pub/sub)
‚îÇ   ‚îî‚îÄ‚îÄ types.ts              # TypeScript types
‚îú‚îÄ‚îÄ web/                       # React web dashboard
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx           # Main app with routing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.css           # Dashboard styles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ client.ts     # API client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pages/            # Dashboard pages
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ HomePage.tsx        # Room overview
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ AgentsPage.tsx      # Agent discovery
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ AnalyticsPage.tsx   # Performance analytics
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ RecommendationsPage.tsx # Intelligent recommendations
‚îÇ   ‚îî‚îÄ‚îÄ dist/                 # Built dashboard (served by HTTP server)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/                 # Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/          # Integration tests
‚îú‚îÄ‚îÄ launch.sh                  # Launch script
‚îú‚îÄ‚îÄ stop.sh                    # Stop script
‚îî‚îÄ‚îÄ memory-bank/               # Documentation
```

## Architecture

### Communication Flow

```
Agent 1 (Architect) ‚îÄ‚îÄ‚îê
                      ‚îú‚îÄ‚îÄ> WebSocket Server ‚îÄ‚îÄ> Room ‚îÄ‚îÄ> Broadcast
Agent 2 (Critic)    ‚îÄ‚îÄ‚î§
                      ‚îÇ
Agent 3 (Pragmatist)‚îÄ‚îÄ‚îò
```

### Components

**Server (`server/`)**
- WebSocket server on port 8080
- Room manages agents and message broadcasting
- Protocol validation with Zod schemas

**Agent (`agent/`)**
- Runtime orchestrates Client + LLM
- Client handles WebSocket connection and events
- LLM wrapper for Ollama with role-specific prompts

**Shared (`shared/`)**
- Protocol definitions (JOIN, MESSAGE, LEAVE, etc.)
- Type-safe message validation
- Shared TypeScript types

### Agent Roles

**Architect** - Designs high-level system architecture, thinks about scalability and maintainability, proposes structural solutions.

**Critic** - Questions assumptions, identifies risks and edge cases, challenges proposed solutions constructively.

**Pragmatist** - Focuses on practical implementation, balances ideals with constraints, proposes actionable next steps.

## Example Conversation

Topic: "Should we use microservices or monolith?"

**Alice (Architect)**: "Start with a modular monolith using bounded contexts and clear CI/CD pipeline, then progressively extract services only when specific domains hit real scalability or team autonomy thresholds."

**Bob (Critic)**: "The assumption that bounded contexts always map cleanly to later services is risky‚Äîinternal coupling can creep in, making eventual extraction painful."

**Charlie (Pragmatist)**: "Use explicit domain-driven design, automated tests, and a CI gate that rejects cross-module API changes. Track concrete metrics and define thresholds that trigger extraction."

## Configuration

### Ollama Setup

The default configuration expects Ollama at `http://192.168.1.4:11434` with model `gpt-oss:20b`.

To use different settings:

```bash
# Option 1: Environment variables
export OLLAMA_HOST=http://localhost:11434
export OLLAMA_MODEL=llama3
./launch.sh

# Option 2: CLI flags
node dist/agent/index.js \
  --id alice \
  --name Alice \
  --role architect \
  --ollama-host http://localhost:11434 \
  --ollama-model llama3
```

### Port Configuration

Server uses port 8080 by default. To change:

```bash
PORT=9000 node dist/server/index.js "Your topic"
```

Then connect agents with:

```bash
node dist/agent/index.js \
  --id alice \
  --name Alice \
  --role architect \
  --server ws://localhost:9000
```

## Cross-Server Communication

PingPong supports running multiple server instances that can communicate with each other via a shared Message Bus (pub/sub pattern). This enables:
- **Cross-project communication** - Agents in Project A can talk to agents in Project B
- **Horizontal scaling** - Distribute load across multiple servers
- **High availability** - Continue operation if one server fails
- **Zero agent impact** - Agents are completely unaware of the multi-server topology

### How It Works

When multiple servers share a Message Bus:
1. Agent sends message to their local server
2. Server broadcasts to local agents AND publishes to Message Bus
3. Other servers receive from Message Bus and broadcast to their local agents
4. All agents see all messages, regardless of which server they're connected to

```
Agent A (Server 1) ‚îÄ‚îÄ> Server 1 ‚îÄ‚îÄ‚îê
                                   ‚îú‚îÄ‚îÄ> Message Bus ‚îÄ‚îÄ‚îê
Agent B (Server 2) ‚îÄ‚îÄ> Server 2 ‚îÄ‚îÄ‚îò                   ‚îÇ
                                                       ‚îÇ
Agent C (Server 1) <‚îÄ‚îÄ Server 1 <‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                                                    ‚îÇ  ‚îÇ
Agent D (Server 2) <‚îÄ‚îÄ Server 2 <‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò
```

**Note**: Only MESSAGE events cross servers. JOIN and LEAVE events remain server-local for performance.

### InMemoryMessageBus (Local Development)

For local development on the same machine:

```bash
# Terminal 1: Start multi-server test script
npx tsx test-multi-server.ts
```

This starts two servers (8081, 8082) with a shared in-memory message bus.

```bash
# Terminal 2: Connect agent to Server 1
node dist/agent/index.js \
  --id alice \
  --name Alice \
  --role architect \
  --server ws://localhost:8081

# Terminal 3: Connect agent to Server 2
node dist/agent/index.js \
  --id bob \
  --name Bob \
  --role critic \
  --server ws://localhost:8082
```

Alice and Bob will see each other's messages even though they're connected to different servers.

**InMemoryMessageBus Limitations:**
- Only works within a single process (same machine)
- Cannot communicate across different machines
- Not suitable for production distributed deployments

### RedisMessageBus (Production)

**Status**: ‚úÖ Implemented

For production deployments across multiple machines using Redis pub/sub:

```bash
# Enable MessageBus with Redis URL via environment variable
MESSAGE_BUS=redis://localhost:6379 node dist/server/index.js "Your topic"

# Or programmatically with RedisMessageBus instance
import { startServer } from './server/index.js';
import { RedisMessageBus } from './shared/message-bus.js';

const bus = new RedisMessageBus('redis://localhost:6379');
await bus.connect();

await startServer(8080, 'Topic', { bus });

# Or via startServer config
await startServer(8080, 'Topic', {
  redisUrl: 'redis://localhost:6379',
  serverId: 'server-1' // optional
});
```

**RedisMessageBus Features:**
- ‚úÖ Cross-machine communication via Redis pub/sub
- ‚úÖ Production-grade reliability with ioredis
- ‚úÖ Automatic JSON serialization/deserialization
- ‚úÖ Error isolation (subscriber errors don't affect others)
- ‚úÖ Channel-based topic isolation
- ‚úÖ Echo filtering (servers don't receive their own messages)
- ‚úÖ Message deduplication support

**Redis Setup:**
```bash
# Local development with Docker
docker run -d -p 6379:6379 redis

# Or install Redis locally
brew install redis
redis-server

# Production: Use managed Redis (AWS ElastiCache, Redis Cloud, etc.)
```

### Manual Testing

Use the included test script to experiment with multi-server communication:

```bash
# Build the project
npm run build

# Start the multi-server test
npx tsx test-multi-server.ts

# In separate terminals, connect agents to different servers
node dist/agent/index.js --id alice --name Alice --role architect --server ws://localhost:8081
node dist/agent/index.js --id bob --name Bob --role critic --server ws://localhost:8082

# Watch as messages cross server boundaries!
```

### Integration Tests

Multi-server communication is thoroughly tested:

```bash
# Run InMemoryMessageBus multi-server tests
npm test -- tests/integration/multi-server.test.ts

# Run RedisMessageBus integration tests (requires Redis)
docker run -d -p 6379:6379 redis
npm test -- tests/integration/redis-multi-server.test.ts

# Run all tests including multi-server
npm test -- --run
```

**InMemoryMessageBus Tests** (5 tests):
- Agents connecting to different servers
- Messages crossing from server 1 to server 2
- Messages crossing from server 2 to server 1
- Bidirectional conversation between servers
- Echo filtering (servers don't receive their own messages back)

**RedisMessageBus Tests** (5 tests):
- Same scenarios as InMemoryMessageBus but with real Redis
- Tests automatically skip if Redis is not available
- Validates cross-machine communication via Redis pub/sub

### Architecture Details

**Message Bus Interface** (`shared/message-bus.ts`):
- Abstract `MessageBus` interface for pluggable implementations
- `BusMessage` format with serverId, messageId, timestamp, payload
- Message deduplication using messageId
- Echo filtering using serverId

**Room Integration** (`server/room.ts`):
- Optional MessageBus injection via constructor
- Publishes MESSAGE events to bus after local broadcast
- Subscribes to bus and delivers messages to local agents
- Filters echo messages (same serverId)
- Deduplicates messages (seen messageId)

**Server Configuration** (`server/index.ts`):
- Optional busConfig parameter
- Supports shared bus instance (for testing)
- Supports MESSAGE_BUS environment variable
- Auto-generates unique serverId per instance

## REST API

PingPong includes a comprehensive REST API for programmatic access to rooms, agents, analytics, and conversation data.

**Base URL**: `http://localhost:3000/api`

### Rooms

**GET /api/rooms**
List all conversation rooms

```bash
curl http://localhost:3000/api/rooms
```

Response:
```json
{
  "rooms": [
    {
      "id": "room-123",
      "topic": "Should we use microservices?",
      "mode": "deep",
      "agentCount": 3,
      "messageCount": 42
    }
  ],
  "total": 1
}
```

**GET /api/rooms/:roomId**
Get detailed room information including quality metrics and conversation summary

```bash
curl http://localhost:3000/api/rooms/room-123
```

### Agents

**GET /api/agents**
Discover all active agents across all rooms

```bash
curl http://localhost:3000/api/agents
```

Response:
```json
{
  "agents": [
    {
      "id": "alice-1",
      "name": "Alice",
      "role": "architect",
      "roomId": "room-123",
      "roomTopic": "Should we use microservices?",
      "metadata": {
        "capabilities": ["PROPOSE", "ANALYZE", "DECISION_MAKING"],
        "personality": {
          "creativity": 0.8,
          "analyticalDepth": 0.9
        }
      }
    }
  ],
  "total": 3
}
```

**GET /api/agents/:agentId**
Get detailed information about a specific agent

### Analytics

**GET /api/analytics/rooms/:roomId**
Get room-wide analytics and quality metrics

```bash
curl http://localhost:3000/api/analytics/rooms/room-123
```

**GET /api/analytics/rooms/:roomId/agents**
Get performance metrics for all agents in a room

```bash
curl http://localhost:3000/api/analytics/rooms/room-123/agents
```

Response:
```json
{
  "roomId": "room-123",
  "metrics": [
    {
      "agentId": "alice-1",
      "agentName": "Alice",
      "totalMessages": 15,
      "totalVotes": 8,
      "engagementScore": 7.5,
      "influenceScore": 6.2
    }
  ]
}
```

**GET /api/analytics/rooms/:roomId/leaderboard?metric=engagement&limit=10**
Get agent leaderboard sorted by engagement or influence

### Export

**GET /api/export/:roomId?format=json|markdown|html**
Export conversation in various formats

```bash
# Export as JSON
curl http://localhost:3000/api/export/room-123?format=json -o conversation.json

# Export as Markdown
curl http://localhost:3000/api/export/room-123?format=markdown -o conversation.md

# Export as HTML
curl http://localhost:3000/api/export/room-123?format=html -o conversation.html
```

**POST /api/export/batch**
Batch export multiple rooms

```bash
curl -X POST http://localhost:3000/api/export/batch \
  -H "Content-Type: application/json" \
  -d '{"roomIds": ["room-1", "room-2"], "format": "markdown"}'
```

### Recommendations

**GET /api/recommendations/:roomId**
Get intelligent agent recommendations based on conversation analysis

```bash
curl http://localhost:3000/api/recommendations/room-123
```

Response:
```json
{
  "roomId": "room-123",
  "conversationNeeds": {
    "conversationQuality": 65,
    "participationBalance": 0.75,
    "missingCapabilities": ["MODERATE", "CODE_REVIEW"],
    "bottlenecks": ["unbalanced_participation"]
  },
  "recommendations": [
    {
      "recommendedRole": "moderator",
      "recommendedCapabilities": ["MODERATE", "SUMMARIZE"],
      "reason": "Conversation would benefit from better structure and summarization",
      "priority": "medium"
    }
  ],
  "recommendationCount": 1
}
```

### Port Configuration

The HTTP server uses port 3000 by default. To change:

```bash
HTTP_PORT=4000 node dist/server/index.js "Your topic"
```

## Web Dashboard

PingPong includes a modern React-based web dashboard for monitoring conversations, discovering agents, viewing analytics, and getting intelligent recommendations.

### Accessing the Dashboard

Once the server is running, open your browser to:

```
http://localhost:3000
```

The dashboard is automatically served by the HTTP API server.

### Dashboard Features

**1. Home - Room Overview**
- View all active conversation rooms
- See agent counts and message counts
- Export conversations to JSON, Markdown, or HTML with one click

**2. Agent Discovery**
- Browse all active agents across all rooms
- View agent roles, capabilities, and metadata
- See which room each agent is participating in
- Filter by capabilities and personality traits

**3. Analytics Dashboard**
- Interactive performance charts (powered by Recharts)
- View engagement scores and influence scores
- Track message counts and vote participation
- Compare agent performance side-by-side
- Leaderboards for top-performing agents

**4. Intelligent Recommendations**
- AI-powered conversation analysis
- See conversation quality scores (0-100)
- Identify participation imbalances
- Discover missing capabilities
- Get prioritized recommendations for which agents to add
- View detailed reasoning for each recommendation

### Building the Dashboard

The dashboard is built during the main project build:

```bash
npm run build
```

This builds both the server TypeScript code and the React dashboard.

For development with hot reload:

```bash
# Terminal 1: Start the API server
node dist/server/index.js "Your topic"

# Terminal 2: Start the dashboard dev server
cd web
npm run dev
```

The dev server runs on `http://localhost:5173` and proxies API requests to port 3000.

## Logs

When using `launch.sh`, logs are stored in `.logs/`:

```bash
# View all logs
tail -f .logs/*.log

# View server logs only
tail -f .logs/server.log

# View specific agent logs
tail -f .logs/agent-alice-1.log
```

## Troubleshooting

### Port Already in Use

```bash
# Check what's using port 8080
lsof -i :8080

# Kill the process
kill -9 $(lsof -t -i :8080)

# Or use the stop script
./stop.sh
```

### Ollama Connection Errors

```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama
ollama serve

# Verify model is installed
ollama list
```

### Agents Not Sending Messages

- Ensure Ollama is running and model is loaded
- Check `.logs/agent-*.log` for errors
- Verify Ollama host/model configuration
- Try with a smaller, faster model like `llama3:8b`

## Testing Notes

### Integration Tests

Integration tests use real Ollama and can be slower. They validate:
- Multi-agent conversation flow
- Agent joining/leaving dynamics
- Message broadcasting
- Conversation quality

### Skipping Slow Tests

```bash
# Run only unit tests (fast)
npm test tests/unit/

# Skip integration tests
npm test -- --exclude tests/integration/
```

## Contributing

This project was built using strict TDD methodology:
1. Write failing test (RED)
2. Implement minimal code to pass (GREEN)
3. Refactor while keeping tests green (REFACTOR)

When adding features:
- Write tests first
- Keep files under 900 lines
- Use real connections (no mocks) in integration tests
- Validate with manual testing after automated tests pass

## License

MIT

## Status

**Milestone 1: Complete** ‚úÖ

- [x] Project setup
- [x] Protocol implementation (Zod validation)
- [x] Server implementation (WebSocket + Room)
- [x] Agent implementation (Client + LLM + Runtime)
- [x] Cross-server communication (MessageBus abstraction)
- [x] InMemoryMessageBus (local development)
- [x] RedisMessageBus (production, cross-machine)
- [x] Integration tests (95 passing, 5 skip without Redis)
- [x] Manual validation (real multi-agent conversations)
- [x] Launch/stop scripts
- [x] Race condition fixes

**Advanced Features: Complete** ‚úÖ

- [x] Agent Metadata system (capabilities, personality traits, LLM config)
- [x] Conversation Export (JSON, Markdown, HTML formats)
- [x] Performance Analytics (engagement scores, influence metrics)
- [x] REST API (Express server with comprehensive endpoints)
- [x] Web Dashboard (React-based UI with routing)
- [x] Agent Discovery UI (browse agents, view metadata)
- [x] Analytics Dashboard (interactive charts with Recharts)
- [x] Intelligent Recommendations (AI-powered conversation analysis)

**Next**: Milestone 2 - Multi-room support, agent memory, consensus mechanisms
